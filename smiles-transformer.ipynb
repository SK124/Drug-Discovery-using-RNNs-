{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SMILES Transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1RgP_8KO5ASf6ywA5UXi9oB6gdZe3ev2Z",
      "authorship_tag": "ABX9TyP2IGy7B/QV2nkOlZ1gIJKP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/SK124/29364eb2ab7cc5816d5aa9f9e1cac09b/smiles-transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJdfpgTocLzK",
        "outputId": "3977ef00-ec8c-4a53-a285-dc488ce2fcef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!git clone https://github.com/DSPsleeporg/smiles-transformer.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'smiles-transformer'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects:   4% (1/21)\u001b[K\rremote: Counting objects:   9% (2/21)\u001b[K\rremote: Counting objects:  14% (3/21)\u001b[K\rremote: Counting objects:  19% (4/21)\u001b[K\rremote: Counting objects:  23% (5/21)\u001b[K\rremote: Counting objects:  28% (6/21)\u001b[K\rremote: Counting objects:  33% (7/21)\u001b[K\rremote: Counting objects:  38% (8/21)\u001b[K\rremote: Counting objects:  42% (9/21)\u001b[K\rremote: Counting objects:  47% (10/21)\u001b[K\rremote: Counting objects:  52% (11/21)\u001b[K\rremote: Counting objects:  57% (12/21)\u001b[K\rremote: Counting objects:  61% (13/21)\u001b[K\rremote: Counting objects:  66% (14/21)\u001b[K\rremote: Counting objects:  71% (15/21)\u001b[K\rremote: Counting objects:  76% (16/21)\u001b[K\rremote: Counting objects:  80% (17/21)\u001b[K\rremote: Counting objects:  85% (18/21)\u001b[K\rremote: Counting objects:  90% (19/21)\u001b[K\rremote: Counting objects:  95% (20/21)\u001b[K\rremote: Counting objects: 100% (21/21)\u001b[K\rremote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 620 (delta 9), reused 6 (delta 2), pack-reused 599\u001b[K\n",
            "Receiving objects: 100% (620/620), 72.26 MiB | 32.28 MiB/s, done.\n",
            "Resolving deltas: 100% (417/417), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b77PvLpStZpC",
        "outputId": "6bc15043-9c0f-4f71-9625-a43fdedf6446",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd /content/smiles-transformer/smiles_transformer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/smiles-transformer/smiles_transformer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Mj7aX2qvF8q"
      },
      "source": [
        "cp /content/drive/'My Drive'/Miniconda3-latest-Linux-x86_64.sh /content/ "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmuJ82CJvVQZ",
        "outputId": "5a95c0b5-05da-425b-8d27-a1ddd56af1d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!time bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "!time conda install -q -y -c conda-forge rdkit\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "sys.path.append('/usr/local/lib/python3.7/site-packages/rdkit')\n",
        "%cd /usr/local/lib/python3.7/site-packages/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PREFIX=/usr/local\n",
            "Unpacking payload ...\n",
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - _libgcc_mutex==0.1=main\n",
            "    - ca-certificates==2020.1.1=0\n",
            "    - certifi==2020.4.5.1=py37_0\n",
            "    - cffi==1.14.0=py37he30daa8_1\n",
            "    - chardet==3.0.4=py37_1003\n",
            "    - conda-package-handling==1.6.1=py37h7b6447c_0\n",
            "    - conda==4.8.3=py37_0\n",
            "    - cryptography==2.9.2=py37h1ba5d50_0\n",
            "    - idna==2.9=py_1\n",
            "    - ld_impl_linux-64==2.33.1=h53a641e_7\n",
            "    - libedit==3.1.20181209=hc058e9b_0\n",
            "    - libffi==3.3=he6710b0_1\n",
            "    - libgcc-ng==9.1.0=hdf63c60_0\n",
            "    - libstdcxx-ng==9.1.0=hdf63c60_0\n",
            "    - ncurses==6.2=he6710b0_1\n",
            "    - openssl==1.1.1g=h7b6447c_0\n",
            "    - pip==20.0.2=py37_3\n",
            "    - pycosat==0.6.3=py37h7b6447c_0\n",
            "    - pycparser==2.20=py_0\n",
            "    - pyopenssl==19.1.0=py37_0\n",
            "    - pysocks==1.7.1=py37_0\n",
            "    - python==3.7.7=hcff3b4d_5\n",
            "    - readline==8.0=h7b6447c_0\n",
            "    - requests==2.23.0=py37_0\n",
            "    - ruamel_yaml==0.15.87=py37h7b6447c_0\n",
            "    - setuptools==46.4.0=py37_0\n",
            "    - six==1.14.0=py37_0\n",
            "    - sqlite==3.31.1=h62c20be_1\n",
            "    - tk==8.6.8=hbc83047_0\n",
            "    - tqdm==4.46.0=py_0\n",
            "    - urllib3==1.25.8=py37_0\n",
            "    - wheel==0.34.2=py37_0\n",
            "    - xz==5.2.5=h7b6447c_0\n",
            "    - yaml==0.1.7=had09818_2\n",
            "    - zlib==1.2.11=h7b6447c_3\n",
            "\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  _libgcc_mutex      pkgs/main/linux-64::_libgcc_mutex-0.1-main\n",
            "  ca-certificates    pkgs/main/linux-64::ca-certificates-2020.1.1-0\n",
            "  certifi            pkgs/main/linux-64::certifi-2020.4.5.1-py37_0\n",
            "  cffi               pkgs/main/linux-64::cffi-1.14.0-py37he30daa8_1\n",
            "  chardet            pkgs/main/linux-64::chardet-3.0.4-py37_1003\n",
            "  conda              pkgs/main/linux-64::conda-4.8.3-py37_0\n",
            "  conda-package-han~ pkgs/main/linux-64::conda-package-handling-1.6.1-py37h7b6447c_0\n",
            "  cryptography       pkgs/main/linux-64::cryptography-2.9.2-py37h1ba5d50_0\n",
            "  idna               pkgs/main/noarch::idna-2.9-py_1\n",
            "  ld_impl_linux-64   pkgs/main/linux-64::ld_impl_linux-64-2.33.1-h53a641e_7\n",
            "  libedit            pkgs/main/linux-64::libedit-3.1.20181209-hc058e9b_0\n",
            "  libffi             pkgs/main/linux-64::libffi-3.3-he6710b0_1\n",
            "  libgcc-ng          pkgs/main/linux-64::libgcc-ng-9.1.0-hdf63c60_0\n",
            "  libstdcxx-ng       pkgs/main/linux-64::libstdcxx-ng-9.1.0-hdf63c60_0\n",
            "  ncurses            pkgs/main/linux-64::ncurses-6.2-he6710b0_1\n",
            "  openssl            pkgs/main/linux-64::openssl-1.1.1g-h7b6447c_0\n",
            "  pip                pkgs/main/linux-64::pip-20.0.2-py37_3\n",
            "  pycosat            pkgs/main/linux-64::pycosat-0.6.3-py37h7b6447c_0\n",
            "  pycparser          pkgs/main/noarch::pycparser-2.20-py_0\n",
            "  pyopenssl          pkgs/main/linux-64::pyopenssl-19.1.0-py37_0\n",
            "  pysocks            pkgs/main/linux-64::pysocks-1.7.1-py37_0\n",
            "  python             pkgs/main/linux-64::python-3.7.7-hcff3b4d_5\n",
            "  readline           pkgs/main/linux-64::readline-8.0-h7b6447c_0\n",
            "  requests           pkgs/main/linux-64::requests-2.23.0-py37_0\n",
            "  ruamel_yaml        pkgs/main/linux-64::ruamel_yaml-0.15.87-py37h7b6447c_0\n",
            "  setuptools         pkgs/main/linux-64::setuptools-46.4.0-py37_0\n",
            "  six                pkgs/main/linux-64::six-1.14.0-py37_0\n",
            "  sqlite             pkgs/main/linux-64::sqlite-3.31.1-h62c20be_1\n",
            "  tk                 pkgs/main/linux-64::tk-8.6.8-hbc83047_0\n",
            "  tqdm               pkgs/main/noarch::tqdm-4.46.0-py_0\n",
            "  urllib3            pkgs/main/linux-64::urllib3-1.25.8-py37_0\n",
            "  wheel              pkgs/main/linux-64::wheel-0.34.2-py37_0\n",
            "  xz                 pkgs/main/linux-64::xz-5.2.5-h7b6447c_0\n",
            "  yaml               pkgs/main/linux-64::yaml-0.1.7-had09818_2\n",
            "  zlib               pkgs/main/linux-64::zlib-1.2.11-h7b6447c_3\n",
            "\n",
            "\n",
            "Preparing transaction: / \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Executing transaction: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\bdone\n",
            "installation finished.\n",
            "WARNING:\n",
            "    You currently have a PYTHONPATH environment variable set. This may cause\n",
            "    unexpected behavior when running the Python interpreter in Miniconda3.\n",
            "    For best results, please verify that your PYTHONPATH only points to\n",
            "    directories of packages that are compatible with the Python interpreter\n",
            "    in Miniconda3: /usr/local\n",
            "\n",
            "real\t0m31.134s\n",
            "user\t0m13.932s\n",
            "sys\t0m4.621s\n",
            "Collecting package metadata (current_repodata.json): ...working... done\n",
            "Solving environment: ...working... done\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - rdkit\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    boost-1.72.0               |   py37h9de70de_0         316 KB  conda-forge\n",
            "    boost-cpp-1.72.0           |       h7b93d67_2        16.3 MB  conda-forge\n",
            "    bzip2-1.0.8                |       h516909a_2         396 KB  conda-forge\n",
            "    ca-certificates-2020.6.20  |       hecda079_0         145 KB  conda-forge\n",
            "    cairo-1.16.0               |    h3fc0475_1005         1.5 MB  conda-forge\n",
            "    certifi-2020.6.20          |   py37hc8dfbb8_0         151 KB  conda-forge\n",
            "    conda-4.8.3                |   py37hc8dfbb8_1         3.0 MB  conda-forge\n",
            "    fontconfig-2.13.1          |    h1056068_1002         365 KB  conda-forge\n",
            "    freetype-2.10.2            |       he06d7ca_0         905 KB  conda-forge\n",
            "    glib-2.65.0                |       h3eb4bd4_0         2.9 MB\n",
            "    icu-67.1                   |       he1b5a44_0        12.9 MB  conda-forge\n",
            "    jpeg-9d                    |       h516909a_0         266 KB  conda-forge\n",
            "    lcms2-2.11                 |       hbd6801e_0         431 KB  conda-forge\n",
            "    libblas-3.8.0              |      17_openblas          11 KB  conda-forge\n",
            "    libcblas-3.8.0             |      17_openblas          11 KB  conda-forge\n",
            "    libgfortran-ng-7.5.0       |      hdf63c60_11         1.7 MB  conda-forge\n",
            "    libiconv-1.15              |    h516909a_1006         2.0 MB  conda-forge\n",
            "    liblapack-3.8.0            |      17_openblas          11 KB  conda-forge\n",
            "    libopenblas-0.3.10         |pthreads_hb3c22a3_4         7.8 MB  conda-forge\n",
            "    libpng-1.6.37              |       hed695b0_1         308 KB  conda-forge\n",
            "    libtiff-4.1.0              |       hc7e4089_6         668 KB  conda-forge\n",
            "    libuuid-2.32.1             |    h14c3975_1000          26 KB  conda-forge\n",
            "    libwebp-base-1.1.0         |       h516909a_3         845 KB  conda-forge\n",
            "    libxcb-1.13                |    h14c3975_1002         396 KB  conda-forge\n",
            "    libxml2-2.9.10             |       h72b56ed_2         1.3 MB  conda-forge\n",
            "    lz4-c-1.9.2                |       he1b5a44_1         226 KB  conda-forge\n",
            "    numpy-1.19.1               |   py37h8960a57_0         5.2 MB  conda-forge\n",
            "    olefile-0.46               |             py_0          31 KB  conda-forge\n",
            "    openssl-1.1.1g             |       h516909a_1         2.1 MB  conda-forge\n",
            "    pandas-1.1.0               |   py37h3340039_0        10.5 MB  conda-forge\n",
            "    pcre-8.44                  |       he1b5a44_0         261 KB  conda-forge\n",
            "    pillow-7.2.0               |   py37h718be6c_1         675 KB  conda-forge\n",
            "    pixman-0.38.0              |    h516909a_1003         594 KB  conda-forge\n",
            "    pthread-stubs-0.4          |    h14c3975_1001           5 KB  conda-forge\n",
            "    pycairo-1.19.1             |   py37h01af8b0_3          77 KB  conda-forge\n",
            "    python-dateutil-2.8.1      |             py_0         220 KB  conda-forge\n",
            "    python_abi-3.7             |          1_cp37m           4 KB  conda-forge\n",
            "    pytz-2020.1                |     pyh9f0ad1d_0         227 KB  conda-forge\n",
            "    rdkit-2020.03.4            |   py37hdd87690_0        24.6 MB  conda-forge\n",
            "    tk-8.6.10                  |       hed695b0_0         3.2 MB  conda-forge\n",
            "    xorg-kbproto-1.0.7         |    h14c3975_1002          26 KB  conda-forge\n",
            "    xorg-libice-1.0.10         |       h516909a_0          57 KB  conda-forge\n",
            "    xorg-libsm-1.2.3           |    h84519dc_1000          25 KB  conda-forge\n",
            "    xorg-libx11-1.6.9          |       h516909a_0         918 KB  conda-forge\n",
            "    xorg-libxau-1.0.9          |       h14c3975_0          13 KB  conda-forge\n",
            "    xorg-libxdmcp-1.1.3        |       h516909a_0          18 KB  conda-forge\n",
            "    xorg-libxext-1.3.4         |       h516909a_0          51 KB  conda-forge\n",
            "    xorg-libxrender-0.9.10     |    h516909a_1002          31 KB  conda-forge\n",
            "    xorg-renderproto-0.11.1    |    h14c3975_1002           8 KB  conda-forge\n",
            "    xorg-xextproto-7.3.0       |    h14c3975_1002          27 KB  conda-forge\n",
            "    xorg-xproto-7.0.31         |    h14c3975_1007          72 KB  conda-forge\n",
            "    zstd-1.4.5                 |       h6597ccf_2         712 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:       104.2 MB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  boost              conda-forge/linux-64::boost-1.72.0-py37h9de70de_0\n",
            "  boost-cpp          conda-forge/linux-64::boost-cpp-1.72.0-h7b93d67_2\n",
            "  bzip2              conda-forge/linux-64::bzip2-1.0.8-h516909a_2\n",
            "  cairo              conda-forge/linux-64::cairo-1.16.0-h3fc0475_1005\n",
            "  fontconfig         conda-forge/linux-64::fontconfig-2.13.1-h1056068_1002\n",
            "  freetype           conda-forge/linux-64::freetype-2.10.2-he06d7ca_0\n",
            "  glib               pkgs/main/linux-64::glib-2.65.0-h3eb4bd4_0\n",
            "  icu                conda-forge/linux-64::icu-67.1-he1b5a44_0\n",
            "  jpeg               conda-forge/linux-64::jpeg-9d-h516909a_0\n",
            "  lcms2              conda-forge/linux-64::lcms2-2.11-hbd6801e_0\n",
            "  libblas            conda-forge/linux-64::libblas-3.8.0-17_openblas\n",
            "  libcblas           conda-forge/linux-64::libcblas-3.8.0-17_openblas\n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-7.5.0-hdf63c60_11\n",
            "  libiconv           conda-forge/linux-64::libiconv-1.15-h516909a_1006\n",
            "  liblapack          conda-forge/linux-64::liblapack-3.8.0-17_openblas\n",
            "  libopenblas        conda-forge/linux-64::libopenblas-0.3.10-pthreads_hb3c22a3_4\n",
            "  libpng             conda-forge/linux-64::libpng-1.6.37-hed695b0_1\n",
            "  libtiff            conda-forge/linux-64::libtiff-4.1.0-hc7e4089_6\n",
            "  libuuid            conda-forge/linux-64::libuuid-2.32.1-h14c3975_1000\n",
            "  libwebp-base       conda-forge/linux-64::libwebp-base-1.1.0-h516909a_3\n",
            "  libxcb             conda-forge/linux-64::libxcb-1.13-h14c3975_1002\n",
            "  libxml2            conda-forge/linux-64::libxml2-2.9.10-h72b56ed_2\n",
            "  lz4-c              conda-forge/linux-64::lz4-c-1.9.2-he1b5a44_1\n",
            "  numpy              conda-forge/linux-64::numpy-1.19.1-py37h8960a57_0\n",
            "  olefile            conda-forge/noarch::olefile-0.46-py_0\n",
            "  pandas             conda-forge/linux-64::pandas-1.1.0-py37h3340039_0\n",
            "  pcre               conda-forge/linux-64::pcre-8.44-he1b5a44_0\n",
            "  pillow             conda-forge/linux-64::pillow-7.2.0-py37h718be6c_1\n",
            "  pixman             conda-forge/linux-64::pixman-0.38.0-h516909a_1003\n",
            "  pthread-stubs      conda-forge/linux-64::pthread-stubs-0.4-h14c3975_1001\n",
            "  pycairo            conda-forge/linux-64::pycairo-1.19.1-py37h01af8b0_3\n",
            "  python-dateutil    conda-forge/noarch::python-dateutil-2.8.1-py_0\n",
            "  python_abi         conda-forge/linux-64::python_abi-3.7-1_cp37m\n",
            "  pytz               conda-forge/noarch::pytz-2020.1-pyh9f0ad1d_0\n",
            "  rdkit              conda-forge/linux-64::rdkit-2020.03.4-py37hdd87690_0\n",
            "  xorg-kbproto       conda-forge/linux-64::xorg-kbproto-1.0.7-h14c3975_1002\n",
            "  xorg-libice        conda-forge/linux-64::xorg-libice-1.0.10-h516909a_0\n",
            "  xorg-libsm         conda-forge/linux-64::xorg-libsm-1.2.3-h84519dc_1000\n",
            "  xorg-libx11        conda-forge/linux-64::xorg-libx11-1.6.9-h516909a_0\n",
            "  xorg-libxau        conda-forge/linux-64::xorg-libxau-1.0.9-h14c3975_0\n",
            "  xorg-libxdmcp      conda-forge/linux-64::xorg-libxdmcp-1.1.3-h516909a_0\n",
            "  xorg-libxext       conda-forge/linux-64::xorg-libxext-1.3.4-h516909a_0\n",
            "  xorg-libxrender    conda-forge/linux-64::xorg-libxrender-0.9.10-h516909a_1002\n",
            "  xorg-renderproto   conda-forge/linux-64::xorg-renderproto-0.11.1-h14c3975_1002\n",
            "  xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-h14c3975_1002\n",
            "  xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-h14c3975_1007\n",
            "  zstd               conda-forge/linux-64::zstd-1.4.5-h6597ccf_2\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates     pkgs/main::ca-certificates-2020.1.1-0 --> conda-forge::ca-certificates-2020.6.20-hecda079_0\n",
            "  certifi              pkgs/main::certifi-2020.4.5.1-py37_0 --> conda-forge::certifi-2020.6.20-py37hc8dfbb8_0\n",
            "  conda                       pkgs/main::conda-4.8.3-py37_0 --> conda-forge::conda-4.8.3-py37hc8dfbb8_1\n",
            "  openssl              pkgs/main::openssl-1.1.1g-h7b6447c_0 --> conda-forge::openssl-1.1.1g-h516909a_1\n",
            "  tk                         pkgs/main::tk-8.6.8-hbc83047_0 --> conda-forge::tk-8.6.10-hed695b0_0\n",
            "\n",
            "\n",
            "Preparing transaction: ...working... done\n",
            "Verifying transaction: ...working... done\n",
            "Executing transaction: ...working... done\n",
            "\n",
            "real\t0m44.474s\n",
            "user\t0m37.571s\n",
            "sys\t0m4.804s\n",
            "/usr/local/lib/python3.7/site-packages\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBoKYmvXvqQz"
      },
      "source": [
        "import re\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import pickle\n",
        "import sys\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import DataStructs\n",
        "from rdkit.Chem import AllChem\n",
        "from rdkit.Chem import RDConfig\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3BOWALUveuX",
        "outputId": "82ec5654-f3a1-423a-c0be-18675297b145",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd /content/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMeiYx5_wlXX",
        "outputId": "74081b30-584d-4bd0-e0ae-7e093ac8ee35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd /content/smiles-transformer/smiles_transformer/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/smiles-transformer/smiles_transformer\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hs2eTiwhcH3J",
        "outputId": "3442649c-4c54-4022-a6f4-6acf71e0855e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import argparse\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "#from utils import splitsss\n",
        "def main():\n",
        "  \n",
        "    default='/content/drive/My Drive/De NovoDrug/full_data_final.csv'\n",
        "    smiles = pd.read_csv(default)['col'].values\n",
        "    with open('chembl24_corpus.txt', 'a') as f:\n",
        "        for sm in tqdm(smiles):\n",
        "            f.write(splits(sm)+'\\n')\n",
        "    print('Built a corpus file!')\n",
        "\n",
        "if __name__=='__main__':\n",
        "    main()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/100000 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            "  2%|▏         | 2482/100000 [00:00<00:03, 24816.95it/s]\u001b[A\u001b[A\n",
            "\n",
            "  4%|▍         | 4463/100000 [00:00<00:04, 23064.81it/s]\u001b[A\u001b[A\n",
            "\n",
            "  7%|▋         | 6837/100000 [00:00<00:04, 23261.42it/s]\u001b[A\u001b[A\n",
            "\n",
            "  9%|▉         | 9003/100000 [00:00<00:03, 22754.47it/s]\u001b[A\u001b[A\n",
            "\n",
            " 11%|█         | 11180/100000 [00:00<00:03, 22449.74it/s]\u001b[A\u001b[A\n",
            "\n",
            " 13%|█▎        | 13469/100000 [00:00<00:03, 22578.46it/s]\u001b[A\u001b[A\n",
            "\n",
            " 16%|█▌        | 15858/100000 [00:00<00:03, 22954.54it/s]\u001b[A\u001b[A\n",
            "\n",
            " 18%|█▊        | 18245/100000 [00:00<00:03, 23220.71it/s]\u001b[A\u001b[A\n",
            "\n",
            " 21%|██        | 20570/100000 [00:00<00:03, 23229.03it/s]\u001b[A\u001b[A\n",
            "\n",
            " 23%|██▎       | 22948/100000 [00:01<00:03, 23388.78it/s]\u001b[A\u001b[A\n",
            "\n",
            " 25%|██▌       | 25257/100000 [00:01<00:03, 23296.85it/s]\u001b[A\u001b[A\n",
            "\n",
            " 28%|██▊       | 27540/100000 [00:01<00:03, 22064.17it/s]\u001b[A\u001b[A\n",
            "\n",
            " 30%|██▉       | 29912/100000 [00:01<00:03, 22535.31it/s]\u001b[A\u001b[A\n",
            "\n",
            " 32%|███▏      | 32249/100000 [00:01<00:02, 22777.96it/s]\u001b[A\u001b[A\n",
            "\n",
            " 35%|███▍      | 34520/100000 [00:01<00:02, 22660.77it/s]\u001b[A\u001b[A\n",
            "\n",
            " 37%|███▋      | 36782/100000 [00:01<00:02, 22569.91it/s]\u001b[A\u001b[A\n",
            "\n",
            " 39%|███▉      | 39157/100000 [00:01<00:02, 22909.75it/s]\u001b[A\u001b[A\n",
            "\n",
            " 41%|████▏     | 41447/100000 [00:01<00:02, 22490.42it/s]\u001b[A\u001b[A\n",
            "\n",
            " 44%|████▎     | 43707/100000 [00:01<00:02, 22522.15it/s]\u001b[A\u001b[A\n",
            "\n",
            " 46%|████▌     | 45987/100000 [00:02<00:02, 22602.44it/s]\u001b[A\u001b[A\n",
            "\n",
            " 48%|████▊     | 48316/100000 [00:02<00:02, 22802.72it/s]\u001b[A\u001b[A\n",
            "\n",
            " 51%|█████     | 50598/100000 [00:02<00:02, 21650.09it/s]\u001b[A\u001b[A\n",
            "\n",
            " 53%|█████▎    | 52968/100000 [00:02<00:02, 22226.07it/s]\u001b[A\u001b[A\n",
            "\n",
            " 55%|█████▌    | 55328/100000 [00:02<00:01, 22620.57it/s]\u001b[A\u001b[A\n",
            "\n",
            " 58%|█████▊    | 57601/100000 [00:02<00:01, 22233.71it/s]\u001b[A\u001b[A\n",
            "\n",
            " 60%|█████▉    | 59834/100000 [00:02<00:01, 22208.10it/s]\u001b[A\u001b[A\n",
            "\n",
            " 62%|██████▏   | 62181/100000 [00:02<00:01, 22569.74it/s]\u001b[A\u001b[A\n",
            "\n",
            " 65%|██████▍   | 64511/100000 [00:02<00:01, 22781.04it/s]\u001b[A\u001b[A\n",
            "\n",
            " 67%|██████▋   | 66839/100000 [00:02<00:01, 22928.10it/s]\u001b[A\u001b[A\n",
            "\n",
            " 69%|██████▉   | 69155/100000 [00:03<00:01, 22996.86it/s]\u001b[A\u001b[A\n",
            "\n",
            " 71%|███████▏  | 71486/100000 [00:03<00:01, 23089.50it/s]\u001b[A\u001b[A\n",
            "\n",
            " 74%|███████▍  | 73797/100000 [00:03<00:01, 22243.21it/s]\u001b[A\u001b[A\n",
            "\n",
            " 76%|███████▌  | 76157/100000 [00:03<00:01, 22631.38it/s]\u001b[A\u001b[A\n",
            "\n",
            " 78%|███████▊  | 78442/100000 [00:03<00:00, 22696.32it/s]\u001b[A\u001b[A\n",
            "\n",
            " 81%|████████  | 80796/100000 [00:03<00:00, 22942.93it/s]\u001b[A\u001b[A\n",
            "\n",
            " 83%|████████▎ | 83106/100000 [00:03<00:00, 22989.69it/s]\u001b[A\u001b[A\n",
            "\n",
            " 85%|████████▌ | 85408/100000 [00:03<00:00, 22807.62it/s]\u001b[A\u001b[A\n",
            "\n",
            " 88%|████████▊ | 87764/100000 [00:03<00:00, 23027.50it/s]\u001b[A\u001b[A\n",
            "\n",
            " 90%|█████████ | 90150/100000 [00:03<00:00, 23271.01it/s]\u001b[A\u001b[A\n",
            "\n",
            " 92%|█████████▏| 92480/100000 [00:04<00:00, 23013.58it/s]\u001b[A\u001b[A\n",
            "\n",
            " 95%|█████████▍| 94784/100000 [00:04<00:00, 22829.97it/s]\u001b[A\u001b[A\n",
            "\n",
            " 97%|█████████▋| 97069/100000 [00:04<00:00, 22127.45it/s]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 100000/100000 [00:04<00:00, 22648.41it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Built a corpus file!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GormzUqbxMXo"
      },
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn as nn\n",
        "from rdkit import Chem\n",
        "from rdkit import rdBase\n",
        "rdBase.DisableLog('rdApp.*')\n",
        "\n",
        "# Split SMILES into words\n",
        "def splits(sm):\n",
        "    '''\n",
        "    function: Split SMILES into words. Care for Cl, Br, Si, Se, Na etc.\n",
        "    input: A SMILES\n",
        "    output: A string with space between words\n",
        "    '''\n",
        "    arr = []\n",
        "    i = 0\n",
        "    while i < len(sm)-1:\n",
        "        if not sm[i] in ['%', 'C', 'B', 'S', 'N', 'R', 'X', 'L', 'A', 'M', \\\n",
        "                        'T', 'Z', 's', 't', 'H', '+', '-', 'K', 'F']:\n",
        "            arr.append(sm[i])\n",
        "            i += 1\n",
        "        elif sm[i]=='%':\n",
        "            arr.append(sm[i:i+3])\n",
        "            i += 3\n",
        "        elif sm[i]=='C' and sm[i+1]=='l':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='C' and sm[i+1]=='a':\n",
        "            arr.append(sm[i:i+2])\n",
        "\n",
        "            i += 2\n",
        "        elif sm[i]=='C' and sm[i+1]=='u':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='B' and sm[i+1]=='r':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='B' and sm[i+1]=='e':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='B' and sm[i+1]=='a':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='B' and sm[i+1]=='i':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='S' and sm[i+1]=='i':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='S' and sm[i+1]=='e':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='S' and sm[i+1]=='r':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='N' and sm[i+1]=='a':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='N' and sm[i+1]=='i':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='R' and sm[i+1]=='b':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='R' and sm[i+1]=='a':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='X' and sm[i+1]=='e':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='L' and sm[i+1]=='i':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='A' and sm[i+1]=='l':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='A' and sm[i+1]=='s':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='A' and sm[i+1]=='g':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='A' and sm[i+1]=='u':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='M' and sm[i+1]=='g':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='M' and sm[i+1]=='n':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='T' and sm[i+1]=='e':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='Z' and sm[i+1]=='n':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='s' and sm[i+1]=='i':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='s' and sm[i+1]=='e':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='t' and sm[i+1]=='e':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='H' and sm[i+1]=='e':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='+' and sm[i+1]=='2':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='+' and sm[i+1]=='3':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='+' and sm[i+1]=='4':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='-' and sm[i+1]=='2':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='-' and sm[i+1]=='3':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='-' and sm[i+1]=='4':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='K' and sm[i+1]=='r':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        elif sm[i]=='F' and sm[i+1]=='e':\n",
        "            arr.append(sm[i:i+2])\n",
        "            i += 2\n",
        "        else:\n",
        "            arr.append(sm[i])\n",
        "            i += 1\n",
        "    if i == len(sm)-1:\n",
        "        arr.append(sm[i])\n",
        "    return ' '.join(arr) \n",
        "\n",
        "# 活性化関数\n",
        "class GELU(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "\n",
        "# 位置情報を考慮したFFN\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.activation = GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(self.activation(self.w_1(x))))\n",
        "    \n",
        "# 正規化層\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
        "\n",
        "\n",
        "class SublayerConnection(nn.Module):\n",
        "    def __init__(self, size, dropout):\n",
        "        super(SublayerConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "# Sample SMILES from probablistic distribution\n",
        "def sample(msms):\n",
        "    ret = []\n",
        "    for msm in msms:\n",
        "        ret.append(torch.multinomial(msm.exp(), 1).squeeze())\n",
        "    return torch.stack(ret)\n",
        "\n",
        "def validity(smiles):\n",
        "    loss = 0\n",
        "    for sm in smiles:\n",
        "        mol = Chem.MolFromSmiles(sm)\n",
        "        if mol is None:\n",
        "            loss += 1\n",
        "    return 1-loss/len(smiles)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "noepcfd8cIh7",
        "outputId": "021d840f-6f47-44ff-d38b-31384d7297a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import argparse\n",
        "import pickle\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "class TorchVocab(object):\n",
        "    \"\"\"\n",
        "    :property freqs: collections.Counter, コーパス中の単語の出現頻度を保持するオブジェクト\n",
        "    :property stoi: collections.defaultdict, string → id の対応を示す辞書\n",
        "    :property itos: collections.defaultdict, id → string の対応を示す辞書\n",
        "    \"\"\"\n",
        "    def __init__(self, counter, max_size=None, min_freq=1, specials=['<pad>', '<oov>'],\n",
        "                 vectors=None, unk_init=None, vectors_cache=None):\n",
        "        \"\"\"\n",
        "        :param counter: collections.Counter, データ中に含まれる単語の頻度を計測するためのcounter\n",
        "        :param max_size: int, vocabularyの最大のサイズ. Noneの場合は最大値なし. defaultはNone\n",
        "        :param min_freq: int, vocabulary中の単語の最低出現頻度. この数以下の出現回数の単語はvocabularyに加えられない.\n",
        "        :param specials: list of str, vocabularyにあらかじめ登録するtoken\n",
        "        :param vectors: list of vectors, 事前学習済みのベクトル. ex)Vocab.load_vectors\n",
        "        \"\"\"\n",
        "        self.freqs = counter\n",
        "        counter = counter.copy()\n",
        "        min_freq = max(min_freq, 1)\n",
        "\n",
        "        self.itos = list(specials)\n",
        "        # special tokensの出現頻度はvocabulary作成の際にカウントされない\n",
        "        for tok in specials:\n",
        "            del counter[tok]\n",
        "\n",
        "        max_size = None if max_size is None else max_size + len(self.itos)\n",
        "\n",
        "        # まず頻度でソートし、次に文字順で並び替える\n",
        "        words_and_frequencies = sorted(counter.items(), key=lambda tup: tup[0])\n",
        "        words_and_frequencies.sort(key=lambda tup: tup[1], reverse=True)\n",
        "        \n",
        "        # 出現頻度がmin_freq未満のものはvocabに加えない\n",
        "        for word, freq in words_and_frequencies:\n",
        "            if freq < min_freq or len(self.itos) == max_size:\n",
        "                break\n",
        "            self.itos.append(word)\n",
        "\n",
        "        # dictのk,vをいれかえてstoiを作成する\n",
        "        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
        "\n",
        "        self.vectors = None\n",
        "        if vectors is not None:\n",
        "            self.load_vectors(vectors, unk_init=unk_init, cache=vectors_cache)\n",
        "        else:\n",
        "            assert unk_init is None and vectors_cache is None\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        if self.freqs != other.freqs:\n",
        "            return False\n",
        "        if self.stoi != other.stoi:\n",
        "            return False\n",
        "        if self.itos != other.itos:\n",
        "            return False\n",
        "        if self.vectors != other.vectors:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    def vocab_rerank(self):\n",
        "        self.stoi = {word: i for i, word in enumerate(self.itos)}\n",
        "\n",
        "    def extend(self, v, sort=False):\n",
        "        words = sorted(v.itos) if sort else v.itos\n",
        "        for w in words:\n",
        "            if w not in self.stoi:\n",
        "                self.itos.append(w)\n",
        "                self.stoi[w] = len(self.itos) - 1\n",
        "\n",
        "\n",
        "class Vocab(TorchVocab):\n",
        "    def __init__(self, counter, max_size=None, min_freq=1):\n",
        "        self.pad_index = 0\n",
        "        self.unk_index = 1\n",
        "        self.eos_index = 2\n",
        "        self.sos_index = 3\n",
        "        self.mask_index = 4\n",
        "        super().__init__(counter, specials=[\"<pad>\", \"<unk>\", \"<eos>\", \"<sos>\", \"<mask>\"], max_size=max_size, min_freq=min_freq)\n",
        "\n",
        "    # override用\n",
        "    def to_seq(self, sentece, seq_len, with_eos=False, with_sos=False) -> list:\n",
        "        pass\n",
        "\n",
        "    # override用\n",
        "    def from_seq(self, seq, join=False, with_pad=False):\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def load_vocab(vocab_path: str) -> 'Vocab':\n",
        "        with open(vocab_path, \"rb\") as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "    def save_vocab(self, vocab_path):\n",
        "        with open(vocab_path, \"wb\") as f:\n",
        "            pickle.dump(self, f)\n",
        "\n",
        "\n",
        "# テキストファイルからvocabを作成する\n",
        "class WordVocab(Vocab):\n",
        "    def __init__(self, texts, max_size=None, min_freq=1):\n",
        "        print(\"Building Vocab\")\n",
        "        counter = Counter()\n",
        "        for line in texts:\n",
        "            if isinstance(line, list):\n",
        "                words = line\n",
        "            else:\n",
        "                words = line.replace(\"\\n\", \"\").replace(\"\\t\", \"\").split()\n",
        "\n",
        "            for word in words:\n",
        "                counter[word] += 1\n",
        "        super().__init__(counter, max_size=max_size, min_freq=min_freq)\n",
        "\n",
        "    def to_seq(self, sentence, seq_len=None, with_eos=False, with_sos=False, with_len=False):\n",
        "        if isinstance(sentence, str):\n",
        "            sentence = sentence.split()\n",
        "\n",
        "        seq = [self.stoi.get(word, self.unk_index) for word in sentence]\n",
        "\n",
        "        if with_eos:\n",
        "            seq += [self.eos_index]  # this would be index 1\n",
        "        if with_sos:\n",
        "            seq = [self.sos_index] + seq\n",
        "\n",
        "        origin_seq_len = len(seq)\n",
        "\n",
        "        if seq_len is None:\n",
        "            pass\n",
        "        elif len(seq) <= seq_len:\n",
        "            seq += [self.pad_index for _ in range(seq_len - len(seq))]\n",
        "        else:\n",
        "            seq = seq[:seq_len]\n",
        "\n",
        "        return (seq, origin_seq_len) if with_len else seq\n",
        "\n",
        "    def from_seq(self, seq, join=False, with_pad=False):\n",
        "        words = [self.itos[idx]\n",
        "                 if idx < len(self.itos)\n",
        "                 else \"<%d>\" % idx\n",
        "                 for idx in seq\n",
        "                 if not with_pad or idx != self.pad_index]\n",
        "\n",
        "        return \" \".join(words) if join else words\n",
        "\n",
        "    @staticmethod\n",
        "    def load_vocab(vocab_path: str) -> 'WordVocab':\n",
        "        with open(vocab_path, \"rb\") as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "\n",
        "def main():\n",
        "\n",
        "    with open('/content/smiles-transformer/smiles_transformer/chembl24_corpus.txt', \"r\", encoding='utf-8') as f:\n",
        "        vocab = WordVocab(f, max_size=None, min_freq=500)\n",
        "\n",
        "    print(\"VOCAB SIZE:\", len(vocab))\n",
        "    #vocab.save_vocab('vocab2.txt')\n",
        "    vocab.save_vocab('vocab2.pkl')\n",
        "\n",
        "if __name__=='__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building Vocab\n",
            "VOCAB SIZE: 33\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4IUsMtIF6qP",
        "outputId": "3c9c00fc-c7c1-496c-8bb9-3a02384554fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "df=pd.read_csv('/content/smiles-transformer/smiles_transformer/gdb08_bert_train.csv')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>canonical_smiles</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CC(=O)NC(Cc1ccccc1)C(=O)NC(C(=O)NC(C)C(=O)NC(C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Cc1c(NC2CC2)nc(C2CC2)nc1N1CCC(C(F)(F)F)CC1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CN1CCC(=C2c3ccccc3Oc3ccccc32)CC1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>CCSc1nsnc1OC1CN2CCC1CC2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>N#Cc1c(NCc2ccc(Cl)cc2)nn2c(=O)cc(Cc3ccccc3)[nH...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                    canonical_smiles\n",
              "0  CC(=O)NC(Cc1ccccc1)C(=O)NC(C(=O)NC(C)C(=O)NC(C...\n",
              "1         Cc1c(NC2CC2)nc(C2CC2)nc1N1CCC(C(F)(F)F)CC1\n",
              "2                   CN1CCC(=C2c3ccccc3Oc3ccccc32)CC1\n",
              "3                            CCSc1nsnc1OC1CN2CCC1CC2\n",
              "4  N#Cc1c(NCc2ccc(Cl)cc2)nn2c(=O)cc(Cc3ccccc3)[nH..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC_qRs-zw3gQ",
        "outputId": "93ad3867-9227-4da5-a986-62b677cd1abf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "python pretrain_trfm.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"pretrain_trfm.py\", line 7, in <module>\n",
            "    import torch\n",
            "ModuleNotFoundError: No module named 'torch'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBOtI-OiD_cg",
        "outputId": "352b727d-c4cf-47c4-a56b-8ec7c6dcff9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "sm=torch.ones(2,2)\n",
        "v=sm.contiguous().view(-1)\n",
        "v.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhwBXwcTEOOe",
        "outputId": "95bc650a-bdb9-4970-d65c-d4e0e2039990",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "output=torch.randn(2,2,3)\n",
        "output.view(-1, 3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.4751, -2.1334, -1.8683],\n",
              "        [-1.7801,  0.1868, -0.7243],\n",
              "        [ 1.8142, -1.1209,  1.6500],\n",
              "        [-0.0722,  1.5646,  1.6088]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfksgoG2EioR",
        "outputId": "30020ebd-16f6-4625-d134-ed98a6d09f42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "import torch.nn.functional as F\n",
        "loss = F.nll_loss(output.view(-1, 3),sm.contiguous().view(-1).long())\n",
        "loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.3757)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7EpIVOlFC1j",
        "outputId": "02cd07cb-8cbf-4a49-e1e0-dabc35d12ad1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "loss"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(-1.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP_hAWCD2Gka",
        "outputId": "d2fc580c-1780-460a-fa36-bdac0f480337",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import argparse\n",
        "import math\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "from build_vocab import WordVocab\n",
        "from dataset import Seq2seqDataset\n",
        "\n",
        "PAD = 0\n",
        "UNK = 1\n",
        "EOS = 2\n",
        "SOS = 3\n",
        "MASK = 4\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function. No batch support?\"\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model) # (T,H)\n",
        "        position = torch.arange(0., max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0., d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], \n",
        "                         requires_grad=False)\n",
        "        return self.dropout(x)\n",
        "\n",
        "class TrfmSeq2seq(nn.Module):\n",
        "    def __init__(self, in_size, hidden_size, out_size, n_layers, dropout=0.1):\n",
        "        super(TrfmSeq2seq, self).__init__()\n",
        "        self.in_size = in_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embed = nn.Embedding(in_size, hidden_size)\n",
        "        self.pe = PositionalEncoding(hidden_size, dropout)\n",
        "        self.trfm = nn.Transformer(d_model=hidden_size, nhead=4, \n",
        "        num_encoder_layers=n_layers, num_decoder_layers=n_layers, dim_feedforward=hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, out_size)\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src: (T,B)\n",
        "        embedded = self.embed(src)  # (T,B,H)\n",
        "        embedded = self.pe(embedded) # (T,B,H)\n",
        "        hidden = self.trfm(embedded, embedded) # (T,B,H)\n",
        "        out = self.out(hidden) # (T,B,V)\n",
        "        out = F.log_softmax(out, dim=2) # (T,B,V)\n",
        "        return out # (T,B,V)\n",
        "\n",
        "    def _encode(self, src):\n",
        "        # src: (T,B)\n",
        "        embedded = self.embed(src)  # (T,B,H)\n",
        "        embedded = self.pe(embedded) # (T,B,H)\n",
        "        output = embedded\n",
        "        for i in range(self.trfm.encoder.num_layers - 1):\n",
        "            output = self.trfm.encoder.layers[i](output, None)  # (T,B,H)\n",
        "        penul = output.detach().numpy()\n",
        "        output = self.trfm.encoder.layers[-1](output, None)  # (T,B,H)\n",
        "        if self.trfm.encoder.norm:\n",
        "            output = self.trfm.encoder.norm(output) # (T,B,H)\n",
        "        output = output.detach().numpy()\n",
        "        # mean, max, first*2\n",
        "        return np.hstack([np.mean(output, axis=0), np.max(output, axis=0), output[0,:,:], penul[0,:,:] ]) # (B,4H)\n",
        "    \n",
        "    def encode(self, src):\n",
        "        # src: (T,B)\n",
        "        batch_size = src.shape[1]\n",
        "        if batch_size<=100:\n",
        "            return self._encode(src)\n",
        "        else: # Batch is too large to load\n",
        "            print('There are {:d} molecules. It will take a little time.'.format(batch_size))\n",
        "            st,ed = 0,100\n",
        "            out = self._encode(src[:,st:ed]) # (B,4H)\n",
        "            while ed<batch_size:\n",
        "                st += 100\n",
        "                ed += 100\n",
        "                out = np.concatenate([out, self._encode(src[:,st:ed])], axis=0)\n",
        "            return out\n",
        "\n",
        "def parse_arguments():\n",
        "    parser = argparse.ArgumentParser(description='Hyperparams')\n",
        "    parser.add_argument('--n_epoch', '-e', type=int, default=5, help='number of epochs')\n",
        "    parser.add_argument('--vocab', '-v', type=str, default='data/vocab.pkl', help='vocabulary (.pkl)')\n",
        "    parser.add_argument('--data', '-d', type=str, default='data/chembl_25.csv', help='train corpus (.csv)')\n",
        "    parser.add_argument('--out-dir', '-o', type=str, default='../result', help='output directory')\n",
        "    parser.add_argument('--name', '-n', type=str, default='ST', help='model name')\n",
        "    parser.add_argument('--seq_len', type=int, default=220, help='maximum length of the paired seqence')\n",
        "    parser.add_argument('--batch_size', '-b', type=int, default=8, help='batch size')\n",
        "    parser.add_argument('--n_worker', '-w', type=int, default=16, help='number of workers')\n",
        "    parser.add_argument('--hidden', type=int, default=256, help='length of hidden vector')\n",
        "    parser.add_argument('--n_layer', '-l', type=int, default=4, help='number of layers')\n",
        "    parser.add_argument('--n_head', type=int, default=4, help='number of attention heads')\n",
        "    parser.add_argument('--lr', type=float, default=1e-4, help='Adam learning rate')\n",
        "    parser.add_argument('--gpu', metavar='N', type=int, nargs='+', help='list of GPU IDs to use')\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate(model, test_loader, vocab):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for b, sm in enumerate(test_loader):\n",
        "        sm = torch.t(sm.cuda()) # (T,B)\n",
        "        with torch.no_grad():\n",
        "            output = model(sm) # (T,B,V)\n",
        "        loss = F.nll_loss(output.view(-1, len(vocab)),\n",
        "                               sm.contiguous().view(-1),\n",
        "                               ignore_index=PAD)\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(test_loader)\n",
        "\n",
        "def main():\n",
        "    #args = parse_arguments()\n",
        "    assert torch.cuda.is_available()\n",
        "\n",
        "    print('Loading dataset...')\n",
        "    vocab = WordVocab.load_vocab('/content/drive/My Drive/vocab.pkl')\n",
        "    dataset = Seq2seqDataset(pd.read_csv('/content/drive/My Drive/De NovoDrug/full_data_final.csv')['col'].values, vocab)\n",
        "    test_size = 10000\n",
        "    train, test = torch.utils.data.random_split(dataset, [len(dataset)-test_size, test_size])\n",
        "    train_loader = DataLoader(train, batch_size=8, shuffle=True, num_workers=16)\n",
        "    test_loader = DataLoader(test, batch_size=8, shuffle=False, num_workers=16)\n",
        "    print('Train size:', len(train))\n",
        "    print('Test size:', len(test))\n",
        "    del dataset, train, test\n",
        "\n",
        "    model = TrfmSeq2seq(len(vocab), 256, len(vocab), 4).cuda()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "    print(model)\n",
        "    print('Total parameters:', sum(p.numel() for p in model.parameters()))\n",
        "\n",
        "    best_loss = None\n",
        "    for e in range(1, 2):\n",
        "        for b, sm in tqdm(enumerate(train_loader)):\n",
        "            sm = torch.t(sm.cuda()) # (T,B)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(sm) # (T,B,V)\n",
        "            loss = F.nll_loss(output.view(-1, len(vocab)),\n",
        "                    sm.contiguous().view(-1), ignore_index=PAD)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if b%1000==0:\n",
        "                print('Train {:3d}: iter {:5d} | loss {:.3f} | ppl {:.3f}'.format(e, b, loss.item(), math.exp(loss.item())))\n",
        "            if b%10000==0:\n",
        "                loss = evaluate(model, test_loader, vocab)\n",
        "                print('Val {:3d}: iter {:5d} | loss {:.3f} | ppl {:.3f}'.format(e, b, loss, math.exp(loss)))\n",
        "                # Save the model if the validation loss is the best we've seen so far.\n",
        "                if not best_loss or loss < best_loss:\n",
        "                    print(\"[!] saving model...\")\n",
        "                    if not os.path.isdir(\".save\"):\n",
        "                        os.makedirs(\".save\")\n",
        "                    torch.save(model.state_dict(), './.save/trfm_new_%d_%d.pkl' % (e,b))\n",
        "                    best_loss = loss\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        main()\n",
        "    except KeyboardInterrupt as e:\n",
        "        print(\"[STOP]\", e)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n",
            "Train size: 90000\n",
            "Test size: 10000\n",
            "TrfmSeq2seq(\n",
            "  (embed): Embedding(45, 256)\n",
            "  (pe): PositionalEncoding(\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (trfm): Transformer(\n",
            "    (encoder): TransformerEncoder(\n",
            "      (layers): ModuleList(\n",
            "        (0): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (1): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (2): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (3): TransformerEncoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (decoder): TransformerDecoder(\n",
            "      (layers): ModuleList(\n",
            "        (0): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (1): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (2): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (3): TransformerDecoderLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
            "          )\n",
            "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
            "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout2): Dropout(p=0.1, inplace=False)\n",
            "          (dropout3): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (out): Linear(in_features=256, out_features=45, bias=True)\n",
            ")\n",
            "Total parameters: 4245037\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train   1: iter     0 | loss 3.911 | ppl 49.932\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "1it [00:37, 37.95s/it]\u001b[A\n",
            "2it [00:38, 26.61s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   1: iter     0 | loss 3.356 | ppl 28.675\n",
            "[!] saving model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "3it [00:38, 18.66s/it]\u001b[A\n",
            "4it [00:38, 13.10s/it]\u001b[A\n",
            "5it [00:38,  9.20s/it]\u001b[A\n",
            "6it [00:38,  6.47s/it]\u001b[A\n",
            "7it [00:38,  4.57s/it]\u001b[A\n",
            "8it [00:38,  3.23s/it]\u001b[A\n",
            "9it [00:38,  2.29s/it]\u001b[A\n",
            "10it [00:38,  1.63s/it]\u001b[A\n",
            "11it [00:39,  1.18s/it]\u001b[A\n",
            "12it [00:39,  1.17it/s]\u001b[A\n",
            "13it [00:39,  1.58it/s]\u001b[A\n",
            "14it [00:39,  2.11it/s]\u001b[A\n",
            "15it [00:39,  2.73it/s]\u001b[A\n",
            "16it [00:39,  3.47it/s]\u001b[A\n",
            "17it [00:39,  4.30it/s]\u001b[A\n",
            "18it [00:39,  5.14it/s]\u001b[A\n",
            "19it [00:39,  6.00it/s]\u001b[A\n",
            "20it [00:40,  6.80it/s]\u001b[A\n",
            "21it [00:40,  7.34it/s]\u001b[A\n",
            "22it [00:40,  7.92it/s]\u001b[A\n",
            "23it [00:40,  8.19it/s]\u001b[A\n",
            "24it [00:40,  8.60it/s]\u001b[A\n",
            "25it [00:40,  8.82it/s]\u001b[A\n",
            "26it [00:40,  9.08it/s]\u001b[A\n",
            "27it [00:40,  9.08it/s]\u001b[A\n",
            "28it [00:40,  9.20it/s]\u001b[A\n",
            "29it [00:40,  9.28it/s]\u001b[A\n",
            "30it [00:41,  9.30it/s]\u001b[A\n",
            "31it [00:41,  9.34it/s]\u001b[A\n",
            "32it [00:41,  9.34it/s]\u001b[A\n",
            "33it [00:41,  9.41it/s]\u001b[A\n",
            "34it [00:41,  9.45it/s]\u001b[A\n",
            "35it [00:41,  9.58it/s]\u001b[A\n",
            "36it [00:41,  9.48it/s]\u001b[A\n",
            "37it [00:41,  9.41it/s]\u001b[A\n",
            "38it [00:41,  9.44it/s]\u001b[A\n",
            "39it [00:42,  9.35it/s]\u001b[A\n",
            "40it [00:42,  9.40it/s]\u001b[A\n",
            "41it [00:42,  9.41it/s]\u001b[A\n",
            "42it [00:42,  9.44it/s]\u001b[A\n",
            "43it [00:42,  9.60it/s]\u001b[A\n",
            "44it [00:42,  9.39it/s]\u001b[A\n",
            "45it [00:42,  9.23it/s]\u001b[A\n",
            "46it [00:42,  8.87it/s]\u001b[A\n",
            "47it [00:42,  8.81it/s]\u001b[A\n",
            "48it [00:43,  8.84it/s]\u001b[A\n",
            "49it [00:43,  8.85it/s]\u001b[A\n",
            "50it [00:43,  8.61it/s]\u001b[A\n",
            "51it [00:43,  8.77it/s]\u001b[A\n",
            "52it [00:43,  9.07it/s]\u001b[A\n",
            "53it [00:43,  9.02it/s]\u001b[A\n",
            "54it [00:43,  9.12it/s]\u001b[A\n",
            "55it [00:43,  9.26it/s]\u001b[A\n",
            "56it [00:43,  9.38it/s]\u001b[A\n",
            "57it [00:44,  9.43it/s]\u001b[A\n",
            "58it [00:44,  9.46it/s]\u001b[A\n",
            "59it [00:44,  9.26it/s]\u001b[A\n",
            "60it [00:44,  9.33it/s]\u001b[A\n",
            "61it [00:44,  9.34it/s]\u001b[A\n",
            "62it [00:44,  9.21it/s]\u001b[A\n",
            "63it [00:44,  9.40it/s]\u001b[A\n",
            "64it [00:44,  9.56it/s]\u001b[A\n",
            "65it [00:44,  9.64it/s]\u001b[A\n",
            "66it [00:44,  9.73it/s]\u001b[A\n",
            "67it [00:45,  9.65it/s]\u001b[A\n",
            "68it [00:45,  9.68it/s]\u001b[A\n",
            "69it [00:45,  9.62it/s]\u001b[A\n",
            "70it [00:45,  9.55it/s]\u001b[A\n",
            "71it [00:45,  9.61it/s]\u001b[A\n",
            "72it [00:45,  9.62it/s]\u001b[A\n",
            "73it [00:45,  9.47it/s]\u001b[A\n",
            "74it [00:45,  9.52it/s]\u001b[A\n",
            "75it [00:45,  9.45it/s]\u001b[A\n",
            "76it [00:46,  9.28it/s]\u001b[A\n",
            "77it [00:46,  9.29it/s]\u001b[A\n",
            "78it [00:46,  9.46it/s]\u001b[A\n",
            "79it [00:46,  9.40it/s]\u001b[A\n",
            "80it [00:46,  9.44it/s]\u001b[A\n",
            "81it [00:46,  9.32it/s]\u001b[A\n",
            "82it [00:46,  9.50it/s]\u001b[A\n",
            "83it [00:46,  9.51it/s]\u001b[A\n",
            "84it [00:46,  9.46it/s]\u001b[A\n",
            "85it [00:46,  9.40it/s]\u001b[A\n",
            "86it [00:47,  9.42it/s]\u001b[A\n",
            "87it [00:47,  9.51it/s]\u001b[A\n",
            "88it [00:47,  9.39it/s]\u001b[A\n",
            "89it [00:47,  9.41it/s]\u001b[A\n",
            "90it [00:47,  9.44it/s]\u001b[A\n",
            "91it [00:47,  9.44it/s]\u001b[A\n",
            "92it [00:47,  9.45it/s]\u001b[A\n",
            "93it [00:47,  9.57it/s]\u001b[A\n",
            "94it [00:47,  9.61it/s]\u001b[A\n",
            "95it [00:48,  9.68it/s]\u001b[A\n",
            "96it [00:48,  9.69it/s]\u001b[A\n",
            "97it [00:48,  9.74it/s]\u001b[A\n",
            "98it [00:48,  9.76it/s]\u001b[A\n",
            "99it [00:48,  9.59it/s]\u001b[A\n",
            "100it [00:48,  9.47it/s]\u001b[A\n",
            "101it [00:48,  9.52it/s]\u001b[A\n",
            "102it [00:48,  9.47it/s]\u001b[A\n",
            "103it [00:48,  9.40it/s]\u001b[A\n",
            "104it [00:48,  9.44it/s]\u001b[A\n",
            "105it [00:49,  9.41it/s]\u001b[A\n",
            "106it [00:49,  9.44it/s]\u001b[A\n",
            "107it [00:49,  8.56it/s]\u001b[A\n",
            "108it [00:49,  8.63it/s]\u001b[A\n",
            "109it [00:49,  8.90it/s]\u001b[A\n",
            "110it [00:49,  9.15it/s]\u001b[A\n",
            "111it [00:49,  9.33it/s]\u001b[A\n",
            "112it [00:49,  9.48it/s]\u001b[A\n",
            "113it [00:49,  9.55it/s]\u001b[A\n",
            "114it [00:50,  9.63it/s]\u001b[A\n",
            "115it [00:50,  9.67it/s]\u001b[A\n",
            "116it [00:50,  9.59it/s]\u001b[A\n",
            "117it [00:50,  9.47it/s]\u001b[A\n",
            "118it [00:50,  9.46it/s]\u001b[A\n",
            "119it [00:50,  9.46it/s]\u001b[A\n",
            "120it [00:50,  9.44it/s]\u001b[A\n",
            "121it [00:50,  9.45it/s]\u001b[A\n",
            "122it [00:50,  9.48it/s]\u001b[A\n",
            "123it [00:51,  9.50it/s]\u001b[A\n",
            "124it [00:51,  9.40it/s]\u001b[A\n",
            "125it [00:51,  9.53it/s]\u001b[A\n",
            "126it [00:51,  9.53it/s]\u001b[A\n",
            "127it [00:51,  9.46it/s]\u001b[A\n",
            "128it [00:51,  9.49it/s]\u001b[A\n",
            "130it [00:51,  9.72it/s]\u001b[A\n",
            "131it [00:51,  9.69it/s]\u001b[A\n",
            "132it [00:51,  9.60it/s]\u001b[A\n",
            "133it [00:52,  9.54it/s]\u001b[A\n",
            "134it [00:52,  9.51it/s]\u001b[A\n",
            "135it [00:52,  9.51it/s]\u001b[A\n",
            "136it [00:52,  9.37it/s]\u001b[A\n",
            "137it [00:52,  9.23it/s]\u001b[A\n",
            "138it [00:52,  9.32it/s]\u001b[A\n",
            "139it [00:52,  9.33it/s]\u001b[A\n",
            "140it [00:52,  9.22it/s]\u001b[A\n",
            "141it [00:52,  9.30it/s]\u001b[A\n",
            "142it [00:53,  9.31it/s]\u001b[A\n",
            "143it [00:53,  9.29it/s]\u001b[A\n",
            "144it [00:53,  9.47it/s]\u001b[A\n",
            "145it [00:53,  9.57it/s]\u001b[A\n",
            "146it [00:53,  9.46it/s]\u001b[A\n",
            "147it [00:53,  9.53it/s]\u001b[A\n",
            "148it [00:53,  9.63it/s]\u001b[A\n",
            "149it [00:53,  9.54it/s]\u001b[A\n",
            "150it [00:53,  9.57it/s]\u001b[A\n",
            "151it [00:53,  9.64it/s]\u001b[A\n",
            "152it [00:54,  9.62it/s]\u001b[A\n",
            "153it [00:54,  9.58it/s]\u001b[A\n",
            "154it [00:54,  9.48it/s]\u001b[A\n",
            "155it [00:54,  9.50it/s]\u001b[A\n",
            "156it [00:54,  9.55it/s]\u001b[A\n",
            "157it [00:54,  9.54it/s]\u001b[A\n",
            "158it [00:54,  9.48it/s]\u001b[A\n",
            "159it [00:54,  9.48it/s]\u001b[A\n",
            "160it [00:54,  9.46it/s]\u001b[A\n",
            "161it [00:55,  9.49it/s]\u001b[A\n",
            "162it [00:55,  9.30it/s]\u001b[A\n",
            "163it [00:55,  9.40it/s]\u001b[A\n",
            "164it [00:55,  9.33it/s]\u001b[A\n",
            "165it [00:55,  9.04it/s]\u001b[A\n",
            "166it [00:55,  9.26it/s]\u001b[A\n",
            "167it [00:55,  9.41it/s]\u001b[A\n",
            "168it [00:55,  9.39it/s]\u001b[A\n",
            "169it [00:55,  9.43it/s]\u001b[A\n",
            "170it [00:55,  9.53it/s]\u001b[A\n",
            "171it [00:56,  9.36it/s]\u001b[A\n",
            "172it [00:56,  9.33it/s]\u001b[A\n",
            "173it [00:56,  9.48it/s]\u001b[A\n",
            "174it [00:56,  9.55it/s]\u001b[A\n",
            "175it [00:56,  9.35it/s]\u001b[A\n",
            "176it [00:56,  9.50it/s]\u001b[A\n",
            "177it [00:56,  9.59it/s]\u001b[A\n",
            "178it [00:56,  9.55it/s]\u001b[A\n",
            "179it [00:56,  9.59it/s]\u001b[A\n",
            "180it [00:57,  9.62it/s]\u001b[A\n",
            "181it [00:57,  9.67it/s]\u001b[A\n",
            "182it [00:57,  9.73it/s]\u001b[A\n",
            "183it [00:57,  9.77it/s]\u001b[A\n",
            "184it [00:57,  9.81it/s]\u001b[A\n",
            "185it [00:57,  9.77it/s]\u001b[A\n",
            "186it [00:57,  9.54it/s]\u001b[A\n",
            "187it [00:57,  9.60it/s]\u001b[A\n",
            "188it [00:57,  9.71it/s]\u001b[A\n",
            "189it [00:57,  9.62it/s]\u001b[A\n",
            "190it [00:58,  9.59it/s]\u001b[A\n",
            "191it [00:58,  9.66it/s]\u001b[A\n",
            "192it [00:58,  9.58it/s]\u001b[A\n",
            "193it [00:58,  9.27it/s]\u001b[A\n",
            "194it [00:58,  9.12it/s]\u001b[A\n",
            "195it [00:58,  9.33it/s]\u001b[A\n",
            "197it [00:58,  9.59it/s]\u001b[A\n",
            "198it [00:58,  9.52it/s]\u001b[A\n",
            "199it [00:59,  9.43it/s]\u001b[A\n",
            "200it [00:59,  9.54it/s]\u001b[A\n",
            "201it [00:59,  9.36it/s]\u001b[A\n",
            "202it [00:59,  9.33it/s]\u001b[A\n",
            "203it [00:59,  9.09it/s]\u001b[A\n",
            "204it [00:59,  9.10it/s]\u001b[A\n",
            "205it [00:59,  9.18it/s]\u001b[A\n",
            "206it [00:59,  9.22it/s]\u001b[A\n",
            "207it [00:59,  9.33it/s]\u001b[A\n",
            "208it [00:59,  9.35it/s]\u001b[A\n",
            "209it [01:00,  9.18it/s]\u001b[A\n",
            "210it [01:00,  9.19it/s]\u001b[A\n",
            "211it [01:00,  9.36it/s]\u001b[A\n",
            "212it [01:00,  9.33it/s]\u001b[A\n",
            "213it [01:00,  8.96it/s]\u001b[A\n",
            "214it [01:00,  8.94it/s]\u001b[A\n",
            "215it [01:00,  8.86it/s]\u001b[A\n",
            "216it [01:00,  9.06it/s]\u001b[A\n",
            "217it [01:00,  9.17it/s]\u001b[A\n",
            "218it [01:01,  9.28it/s]\u001b[A\n",
            "219it [01:01,  9.20it/s]\u001b[A\n",
            "220it [01:01,  9.24it/s]\u001b[A\n",
            "221it [01:01,  9.33it/s]\u001b[A\n",
            "222it [01:01,  9.17it/s]\u001b[A\n",
            "223it [01:01,  8.77it/s]\u001b[A\n",
            "224it [01:01,  8.74it/s]\u001b[A\n",
            "225it [01:01,  8.68it/s]\u001b[A\n",
            "226it [01:01,  8.80it/s]\u001b[A\n",
            "227it [01:02,  9.04it/s]\u001b[A\n",
            "228it [01:02,  9.03it/s]\u001b[A\n",
            "229it [01:02,  9.03it/s]\u001b[A\n",
            "230it [01:02,  9.14it/s]\u001b[A\n",
            "231it [01:02,  9.19it/s]\u001b[A\n",
            "232it [01:02,  9.23it/s]\u001b[A\n",
            "233it [01:02,  9.29it/s]\u001b[A\n",
            "234it [01:02,  9.17it/s]\u001b[A\n",
            "235it [01:02,  9.21it/s]\u001b[A\n",
            "236it [01:03,  9.16it/s]\u001b[A\n",
            "237it [01:03,  9.18it/s]\u001b[A\n",
            "238it [01:03,  9.11it/s]\u001b[A\n",
            "239it [01:03,  9.17it/s]\u001b[A\n",
            "240it [01:03,  9.07it/s]\u001b[A\n",
            "241it [01:03,  8.95it/s]\u001b[A\n",
            "242it [01:03,  8.85it/s]\u001b[A\n",
            "243it [01:03,  8.75it/s]\u001b[A\n",
            "244it [01:03,  8.73it/s]\u001b[A\n",
            "245it [01:04,  8.68it/s]\u001b[A\n",
            "246it [01:04,  8.71it/s]\u001b[A\n",
            "247it [01:04,  8.63it/s]\u001b[A\n",
            "248it [01:04,  8.80it/s]\u001b[A\n",
            "249it [01:04,  8.98it/s]\u001b[A\n",
            "250it [01:04,  9.24it/s]\u001b[A\n",
            "251it [01:04,  9.33it/s]\u001b[A\n",
            "253it [01:04,  9.55it/s]\u001b[A\n",
            "254it [01:05,  9.67it/s]\u001b[A\n",
            "255it [01:05,  9.57it/s]\u001b[A\n",
            "256it [01:05,  9.64it/s]\u001b[A\n",
            "257it [01:05,  9.46it/s]\u001b[A\n",
            "258it [01:05,  9.61it/s]\u001b[A\n",
            "259it [01:05,  9.49it/s]\u001b[A\n",
            "260it [01:05,  9.23it/s]\u001b[A\n",
            "261it [01:05,  9.07it/s]\u001b[A\n",
            "262it [01:05,  9.14it/s]\u001b[A\n",
            "263it [01:05,  9.24it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[STOP] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vsg35OmPLFFy",
        "outputId": "0ea6a8fc-8e93-4387-e38d-03682745bd49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(os.listdir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<built-in function listdir>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtHW5uun2XYP"
      },
      "source": [
        "import sys\n",
        "from io import StringIO\n",
        "from IPython import get_ipython\n",
        "\n",
        "\n",
        "class IpyExit(SystemExit):\n",
        "    \"\"\"Exit Exception for IPython.\n",
        "\n",
        "    Exception temporarily redirects stderr to buffer.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # print(\"exiting\")  # optionally print some message to stdout, too\n",
        "        # ... or do other stuff before exit\n",
        "        sys.stderr = StringIO()\n",
        "\n",
        "    def __del__(self):\n",
        "        sys.stderr.close()\n",
        "        sys.stderr = sys.__stderr__  # restore from backup\n",
        "\n",
        "\n",
        "def ipy_exit():\n",
        "    raise IpyExit\n",
        "\n",
        "\n",
        "if get_ipython():    # ...run with IPython\n",
        "    exit = ipy_exit  # rebind to custom exit\n",
        "else:\n",
        "    exit = exit "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BG3WHgjB3VHD",
        "outputId": "6f98b06c-64e8-4a1d-8ddc-437916d93f04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "\n",
        "import argparse\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description='Convert smi file to csv file')\n",
        "    parser.add_argument('--in_path', '-i', type=str, default='data/GDB/GDB08.smi', help='input file')\n",
        "    parser.add_argument('--out_path_1', '-o1', type=str, default='data/gdb08_bert_train.csv', help='output file (train)')\n",
        "    parser.add_argument('--out_path_2', '-o2', type=str, default='data/gdb08_bert_test.csv', help='output file (test)')\n",
        "    parser.add_argument('--max', '-m', type=int, default=2e6, help='Maximum number of molecules')\n",
        "   #args = parser.parse_args()\n",
        "    print('Input file: {}'.format('/content/drive/My Drive/De NovoDrug/chembl24_cleaned_unique_canon.txt'))\n",
        "    print('Start preprocessing')\n",
        "\n",
        "    smiles = []\n",
        "    with open('/content/drive/My Drive/De NovoDrug/chembl24_cleaned_unique_canon.txt') as f:\n",
        "        lines = f.readlines()\n",
        "    for l in lines:\n",
        "        smiles.append(l.replace('\\n', ''))\n",
        "    del lines\n",
        "    smiles = np.array(smiles)\n",
        "    N = len(smiles)\n",
        "    print('The dataset contains {} molecules'.format(N))\n",
        "    \n",
        "    rands = np.random.choice(N, min(N,256), replace=False)\n",
        "    smiles_train = smiles[rands[:N//2]]\n",
        "    df_train = pd.DataFrame(data=smiles_train, columns=['canonical_smiles'])\n",
        "    df_train.to_csv('gdb08_bert_train.csv', index=False)\n",
        "    del smiles_train, df_train\n",
        "    smiles_test = smiles[rands[N//2:]]\n",
        "    df_test = pd.DataFrame(data=smiles_test, columns=['canonical_smiles'])\n",
        "    df_test.to_csv('gdb08_bert_test.csv', index=False)\n",
        "    print('Each set contains {} molecules'.format(N//2))\n",
        "\n",
        "if __name__=='__main__':\n",
        "    main()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input file: /content/drive/My Drive/De NovoDrug/chembl24_cleaned_unique_canon.txt\n",
            "Start preprocessing\n",
            "The dataset contains 369860 molecules\n",
            "Each set contains 184930 molecules\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_wI-sPiDHHX"
      },
      "source": [
        "pip in"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCFaYUZ5sEoU",
        "outputId": "ab7559cc-2a80-4083-adf9-4f26065c1c83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd /content"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owgxdNVBtP59",
        "outputId": "5eaf716c-0050-401a-ad4b-b10b2ed31432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "trfm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrfmSeq2seq(\n",
              "  (embed): Embedding(45, 256)\n",
              "  (pe): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (trfm): Transformer(\n",
              "    (encoder): TransformerEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (2): TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): TransformerDecoder(\n",
              "      (layers): ModuleList(\n",
              "        (0): TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (multihead_attn): MultiheadAttention(\n",
              "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "          (dropout3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (1): TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (multihead_attn): MultiheadAttention(\n",
              "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "          (dropout3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (2): TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (multihead_attn): MultiheadAttention(\n",
              "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=256, out_features=256, bias=True)\n",
              "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "          (dropout3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (out): Linear(in_features=256, out_features=45, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aN4t_gypuHkJ",
        "outputId": "0c476dbb-d8e2-4284-93e6-92e820a643f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "vocab"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<build_vocab.WordVocab at 0x7f2d6d80cdd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ec31DAtDDy8",
        "outputId": "74fac5f5-cbfe-4535-c209-95a40949e0f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import torch\n",
        "pad_index = 0\n",
        "unk_index = 1\n",
        "eos_index = 2\n",
        "sos_index = 3\n",
        "mask_index = 4\n",
        "\n",
        "vocab = WordVocab.load_vocab('/content/drive/My Drive/vocab (1).pkl')\n",
        "\n",
        "trfm = TrfmSeq2seq(len(vocab), 256, len(vocab), 4)\n",
        "trfm.load_state_dict(torch.load('/content/drive/My Drive/trfm.pkl'))\n",
        "trfm.eval()\n",
        "print('Total parameters:', sum(p.numel() for p in trfm.parameters()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total parameters: 4245037\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J17o0pIevRUW",
        "outputId": "0c7d469a-5960-4a87-ab09-b46edbdfe47a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "rates = 2**np.arange(7)/80\n",
        "print(rates)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.0125 0.025  0.05   0.1    0.2    0.4    0.8   ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qE8hjMYyLnk"
      },
      "source": [
        "def get_inputs(sm):\n",
        "    seq_len = 220\n",
        "    sm = sm.split()\n",
        "    if len(sm)>218:\n",
        "        print('SMILES is too long ({:d})'.format(len(sm)))\n",
        "        sm = sm[:109]+sm[-109:]\n",
        "    ids = [vocab.stoi.get(token, unk_index) for token in sm]\n",
        "    ids = [sos_index] + ids + [eos_index]\n",
        "    seg = [1]*len(ids)\n",
        "    padding = [pad_index]*(seq_len - len(ids))\n",
        "    ids.extend(padding), seg.extend(padding)\n",
        "    return ids, seg\n",
        "\n",
        "def get_array(smiles):\n",
        "    x_id, x_seg = [], []\n",
        "    for sm in smiles:\n",
        "        a,b = get_inputs(sm)\n",
        "        x_id.append(a)\n",
        "        x_seg.append(b)\n",
        "    return torch.tensor(x_id), torch.tensor(x_seg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxi_zauVyQyt"
      },
      "source": [
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "\n",
        "def bit2np(bitvector):\n",
        "    bitstring = bitvector.ToBitString()\n",
        "    intmap = map(int, bitstring)\n",
        "    return np.array(list(intmap))\n",
        "\n",
        "def extract_morgan(smiles, targets):\n",
        "    x,X,y = [],[],[]\n",
        "    for sm,target in zip(smiles,targets):\n",
        "        mol = Chem.MolFromSmiles(sm)\n",
        "        if mol is None:\n",
        "            print(sm)\n",
        "            continue\n",
        "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, 2, 1024) # Morgan (Similar to ECFP4)\n",
        "        x.append(sm)\n",
        "        X.append(bit2np(fp))\n",
        "        y.append(target)\n",
        "    return x,np.array(X),np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUTspPulzDSL",
        "outputId": "1007d895-1321-459a-c469-102d2d347c68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!git clone https://github.com/deepchem/deepchem.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deepchem'...\n",
            "remote: Enumerating objects: 216, done.\u001b[K\n",
            "remote: Counting objects:   0% (1/216)\u001b[K\rremote: Counting objects:   1% (3/216)\u001b[K\rremote: Counting objects:   2% (5/216)\u001b[K\rremote: Counting objects:   3% (7/216)\u001b[K\rremote: Counting objects:   4% (9/216)\u001b[K\rremote: Counting objects:   5% (11/216)\u001b[K\rremote: Counting objects:   6% (13/216)\u001b[K\rremote: Counting objects:   7% (16/216)\u001b[K\rremote: Counting objects:   8% (18/216)\u001b[K\rremote: Counting objects:   9% (20/216)\u001b[K\rremote: Counting objects:  10% (22/216)\u001b[K\rremote: Counting objects:  11% (24/216)\u001b[K\rremote: Counting objects:  12% (26/216)\u001b[K\rremote: Counting objects:  13% (29/216)\u001b[K\rremote: Counting objects:  14% (31/216)\u001b[K\rremote: Counting objects:  15% (33/216)\u001b[K\rremote: Counting objects:  16% (35/216)\u001b[K\rremote: Counting objects:  17% (37/216)\u001b[K\rremote: Counting objects:  18% (39/216)\u001b[K\rremote: Counting objects:  19% (42/216)\u001b[K\rremote: Counting objects:  20% (44/216)\u001b[K\rremote: Counting objects:  21% (46/216)\u001b[K\rremote: Counting objects:  22% (48/216)\u001b[K\rremote: Counting objects:  23% (50/216)\u001b[K\rremote: Counting objects:  24% (52/216)\u001b[K\rremote: Counting objects:  25% (54/216)\u001b[K\rremote: Counting objects:  26% (57/216)\u001b[K\rremote: Counting objects:  27% (59/216)\u001b[K\rremote: Counting objects:  28% (61/216)\u001b[K\rremote: Counting objects:  29% (63/216)\u001b[K\rremote: Counting objects:  30% (65/216)\u001b[K\rremote: Counting objects:  31% (67/216)\u001b[K\rremote: Counting objects:  32% (70/216)\u001b[K\rremote: Counting objects:  33% (72/216)\u001b[K\rremote: Counting objects:  34% (74/216)\u001b[K\rremote: Counting objects:  35% (76/216)\u001b[K\rremote: Counting objects:  36% (78/216)\u001b[K\rremote: Counting objects:  37% (80/216)\u001b[K\rremote: Counting objects:  38% (83/216)\u001b[K\rremote: Counting objects:  39% (85/216)\u001b[K\rremote: Counting objects:  40% (87/216)\u001b[K\rremote: Counting objects:  41% (89/216)\u001b[K\rremote: Counting objects:  42% (91/216)\u001b[K\rremote: Counting objects:  43% (93/216)\u001b[K\rremote: Counting objects:  44% (96/216)\u001b[K\rremote: Counting objects:  45% (98/216)\u001b[K\rremote: Counting objects:  46% (100/216)\u001b[K\rremote: Counting objects:  47% (102/216)\u001b[K\rremote: Counting objects:  48% (104/216)\u001b[K\rremote: Counting objects:  49% (106/216)\u001b[K\rremote: Counting objects:  50% (108/216)\u001b[K\rremote: Counting objects:  51% (111/216)\u001b[K\rremote: Counting objects:  52% (113/216)\u001b[K\rremote: Counting objects:  53% (115/216)\u001b[K\rremote: Counting objects:  54% (117/216)\u001b[K\rremote: Counting objects:  55% (119/216)\u001b[K\rremote: Counting objects:  56% (121/216)\u001b[K\rremote: Counting objects:  57% (124/216)\u001b[K\rremote: Counting objects:  58% (126/216)\u001b[K\rremote: Counting objects:  59% (128/216)\u001b[K\rremote: Counting objects:  60% (130/216)\u001b[K\rremote: Counting objects:  61% (132/216)\u001b[K\rremote: Counting objects:  62% (134/216)\u001b[K\rremote: Counting objects:  63% (137/216)\u001b[K\rremote: Counting objects:  64% (139/216)\u001b[K\rremote: Counting objects:  65% (141/216)\u001b[K\rremote: Counting objects:  66% (143/216)\u001b[K\rremote: Counting objects:  67% (145/216)\u001b[K\rremote: Counting objects:  68% (147/216)\u001b[K\rremote: Counting objects:  69% (150/216)\u001b[K\rremote: Counting objects:  70% (152/216)\u001b[K\rremote: Counting objects:  71% (154/216)\u001b[K\rremote: Counting objects:  72% (156/216)\u001b[K\rremote: Counting objects:  73% (158/216)\u001b[K\rremote: Counting objects:  74% (160/216)\u001b[K\rremote: Counting objects:  75% (162/216)\u001b[K\rremote: Counting objects:  76% (165/216)\u001b[K\rremote: Counting objects:  77% (167/216)\u001b[K\rremote: Counting objects:  78% (169/216)\u001b[K\rremote: Counting objects:  79% (171/216)\u001b[K\rremote: Counting objects:  80% (173/216)\u001b[K\rremote: Counting objects:  81% (175/216)\u001b[K\rremote: Counting objects:  82% (178/216)\u001b[K\rremote: Counting objects:  83% (180/216)\u001b[K\rremote: Counting objects:  84% (182/216)\u001b[K\rremote: Counting objects:  85% (184/216)\u001b[K\rremote: Counting objects:  86% (186/216)\u001b[K\rremote: Counting objects:  87% (188/216)\u001b[K\rremote: Counting objects:  88% (191/216)\u001b[K\rremote: Counting objects:  89% (193/216)\u001b[K\rremote: Counting objects:  90% (195/216)\u001b[K\rremote: Counting objects:  91% (197/216)\u001b[K\rremote: Counting objects:  92% (199/216)\u001b[K\rremote: Counting objects:  93% (201/216)\u001b[K\rremote: Counting objects:  94% (204/216)\u001b[K\rremote: Counting objects:  95% (206/216)\u001b[K\rremote: Counting objects:  96% (208/216)\u001b[K\rremote: Counting objects:  97% (210/216)\u001b[K\rremote: Counting objects:  98% (212/216)\u001b[K\rremote: Counting objects:  99% (214/216)\u001b[K\rremote: Counting objects: 100% (216/216)\u001b[K\rremote: Counting objects: 100% (216/216), done.\u001b[K\n",
            "remote: Compressing objects: 100% (109/109), done.\u001b[K\n",
            "remote: Total 34834 (delta 147), reused 156 (delta 107), pack-reused 34618\u001b[K\n",
            "Receiving objects: 100% (34834/34834), 440.63 MiB | 32.54 MiB/s, done.\n",
            "Resolving deltas: 100% (25745/25745), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCqdJfJMyUW7",
        "outputId": "840271eb-7ad3-45d6-d247-c01819b5e019",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "df = pd.read_csv('/content/deepchem/examples/hiv/HIV.csv')\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(41913, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>smiles</th>\n",
              "      <th>activity</th>\n",
              "      <th>HIV_active</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>CCC1=[O+][Cu-3]2([O+]=C(CC)C1)[O+]=C(CC)CC(CC)...</td>\n",
              "      <td>CI</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>C(=Cc1ccccc1)C1=[O+][Cu-3]2([O+]=C(C=Cc3ccccc3...</td>\n",
              "      <td>CI</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CC(=O)N1c2ccccc2Sc2c1ccc1ccccc21</td>\n",
              "      <td>CI</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Nc1ccc(C=Cc2ccc(N)cc2S(=O)(=O)O)c(S(=O)(=O)O)c1</td>\n",
              "      <td>CI</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>O=S(=O)(O)CCS(=O)(=O)O</td>\n",
              "      <td>CI</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              smiles activity  HIV_active\n",
              "0  CCC1=[O+][Cu-3]2([O+]=C(CC)C1)[O+]=C(CC)CC(CC)...       CI           0\n",
              "1  C(=Cc1ccccc1)C1=[O+][Cu-3]2([O+]=C(C=Cc3ccccc3...       CI           0\n",
              "2                   CC(=O)N1c2ccccc2Sc2c1ccc1ccccc21       CI           0\n",
              "3    Nc1ccc(C=Cc2ccc(N)cc2S(=O)(=O)O)c(S(=O)(=O)O)c1       CI           0\n",
              "4                             O=S(=O)(O)CCS(=O)(=O)O       CI           0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lLmrfsFzW7P",
        "outputId": "3e800ce1-3504-43ea-836d-66f336ed4463",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "df_large = df[np.array(list(map(len, df['smiles'])))>218]\n",
        "\n",
        "keys = ['0', '1']\n",
        "bottom = df_large.groupby('HIV_active').count()['smiles'].values\n",
        "plt.figure(figsize=(2,4))\n",
        "plt.bar(keys, bottom)\n",
        "plt.xlabel('class')\n",
        "plt.ylabel('counts')\n",
        "plt.title('HIV')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 144x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxfqLSBnzawA",
        "outputId": "628bc82c-e377-41e4-ebe1-5c2d16e28ee1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.hist(list(map(len, df_large['smiles'].values)), bins=20)\n",
        "plt.xlabel('SMILES length')\n",
        "plt.ylabel('counts')\n",
        "plt.title('HIV')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5z_QxAJzeCa"
      },
      "source": [
        "df_train = df[np.array(list(map(len, df['smiles'])))<=218]\n",
        "df_test = df[np.array(list(map(len, df['smiles'])))>218]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6tcD7OUzhVR",
        "outputId": "55837456-6ec9-49d8-a62f-86cd476726c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "df_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(41868, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHt9WcSgzlMs"
      },
      "source": [
        "def ablation_hiv(X, X_test, y, y_test, rate, n_repeats):\n",
        "    auc = np.empty(n_repeats)\n",
        "    for i in range(n_repeats):\n",
        "        clf = MLPClassifier(max_iter=1000)\n",
        "        if rate==1:\n",
        "            X_train, y_train = X,y\n",
        "        else:\n",
        "            X_train, _, y_train, __ = train_test_split(X, y, test_size=1-rate, stratify=y)\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_score = clf.predict_proba(X_test)\n",
        "        auc[i] = roc_auc_score(y_test, y_score[:,1])\n",
        "    ret = {}\n",
        "    ret['auc mean'] = np.mean(auc)\n",
        "    ret['auc std'] = np.std(auc)\n",
        "    return ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84s1Y8jjzoqm",
        "outputId": "6ccd29d5-867b-4381-b419-0593bd5b9df1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        }
      },
      "source": [
        "x_split = [splits(sm) for sm in df_train['smiles'].values]\n",
        "xid, _ = get_array(x_split)\n",
        "X = trfm.encode(torch.t(xid))\n",
        "print(X.shape)\n",
        "x_split = [splits(sm) for sm in df_test['smiles'].values]\n",
        "xid, _ = get_array(x_split)\n",
        "X_test = trfm.encode(torch.t(xid))\n",
        "print(X_test.shape)\n",
        "y, y_test = df_train['HIV_active'].values, df_test['HIV_active'].values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 41868 molecules. It will take a little time.\n",
            "(41868, 1024)\n",
            "SMILES is too long (220)\n",
            "SMILES is too long (274)\n",
            "SMILES is too long (247)\n",
            "SMILES is too long (226)\n",
            "SMILES is too long (244)\n",
            "SMILES is too long (243)\n",
            "SMILES is too long (253)\n",
            "SMILES is too long (266)\n",
            "SMILES is too long (346)\n",
            "SMILES is too long (232)\n",
            "SMILES is too long (242)\n",
            "SMILES is too long (247)\n",
            "SMILES is too long (240)\n",
            "SMILES is too long (370)\n",
            "SMILES is too long (224)\n",
            "SMILES is too long (283)\n",
            "SMILES is too long (265)\n",
            "SMILES is too long (240)\n",
            "SMILES is too long (219)\n",
            "SMILES is too long (246)\n",
            "SMILES is too long (243)\n",
            "SMILES is too long (284)\n",
            "SMILES is too long (270)\n",
            "SMILES is too long (232)\n",
            "SMILES is too long (260)\n",
            "SMILES is too long (284)\n",
            "SMILES is too long (284)\n",
            "SMILES is too long (439)\n",
            "SMILES is too long (491)\n",
            "SMILES is too long (439)\n",
            "SMILES is too long (296)\n",
            "SMILES is too long (341)\n",
            "SMILES is too long (285)\n",
            "SMILES is too long (327)\n",
            "SMILES is too long (341)\n",
            "SMILES is too long (400)\n",
            "SMILES is too long (263)\n",
            "SMILES is too long (238)\n",
            "SMILES is too long (383)\n",
            "SMILES is too long (360)\n",
            "SMILES is too long (233)\n",
            "SMILES is too long (365)\n",
            "SMILES is too long (265)\n",
            "SMILES is too long (240)\n",
            "SMILES is too long (223)\n",
            "(45, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4ct7fUj0ZXW",
        "outputId": "141b0799-7ab9-43f0-a04e-c236de5f6777",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "scores = []\n",
        "for rate in rates:\n",
        "    score_dic = ablation_hiv(X, X_test, y, y_test, rate, 20)\n",
        "    print(rate, score_dic)\n",
        "    scores.append(score_dic['auc mean'])\n",
        "print(np.mean(scores))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0125 {'auc mean': 0.586111111111111, 'auc std': 0.12544274585413737}\n",
            "0.025 {'auc mean': 0.5390432098765432, 'auc std': 0.10447553659000973}\n",
            "0.05 {'auc mean': 0.5945987654320988, 'auc std': 0.11715504205531173}\n",
            "0.1 {'auc mean': 0.5652777777777777, 'auc std': 0.12755271450619823}\n",
            "0.2 {'auc mean': 0.6206790123456789, 'auc std': 0.14588321375706326}\n",
            "0.4 {'auc mean': 0.7348765432098766, 'auc std': 0.09442073857252123}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:573: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:573: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:573: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzNcqA6J1TjE"
      },
      "source": [
        "\n",
        "score_dic = ablation_hiv(X, X_test, y, y_test, 1, 20)\n",
        "print(score_dic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJtnus9O5rTs"
      },
      "source": [
        "x,X,y = extract_morgan(df_train['smiles'].values, df_train['HIV_active'].values)\n",
        "print(len(X), len(y))\n",
        "x,X_test,y_test = extract_morgan(df_test['smiles'].values, df_test['HIV_active'].values)\n",
        "print(len(X_test), len(y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oocshZlO5vOw"
      },
      "source": [
        "scores = []\n",
        "for rate in rates:\n",
        "    score_dic = ablation_hiv(X, X_test, y, y_test, rate, 20)\n",
        "    print(rate, score_dic)\n",
        "    scores.append(score_dic['auc mean'])\n",
        "print(np.mean(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2UkQ0Pb5y4q"
      },
      "source": [
        "score_dic = ablation_hiv(X, X_test, y, y_test, 1, 20)\n",
        "print(score_dic)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}